{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7284\\1913879155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_list = ['s/o','d/o','w/o','/','&',',','-']\n",
    "\n",
    "def clean_data(name):\n",
    "\tname = str(name).lower()\n",
    "\tname = (''.join(i for i in name if ord(i)<128)).strip()\n",
    "\tfor repl in repl_list:\n",
    "\t\tname = name.replace(repl,\" \")\n",
    "\tif '@' in name:\n",
    "\t\tpos = name.find('@')\n",
    "\t\tname = name[:pos].strip()\n",
    "\tname = name.split(\" \")\n",
    "\tname = \" \".join([each.strip() for each in name])\n",
    "\treturn name\n",
    "\n",
    "def remove_records(merged_data):\n",
    "\tmerged_data['delete'] = 0\n",
    "\tmerged_data.loc[merged_data['name'].str.find('with') != -1,'delete'] = 1\t\n",
    "\tmerged_data.loc[merged_data['count_words']>=5,'delete']=1\n",
    "\tmerged_data.loc[merged_data['count_words']==0,'delete']=1\n",
    "\tmerged_data.loc[merged_data['name'].str.contains(r'\\d') == True,'delete']=1\n",
    "\tcleaned_data = merged_data[merged_data.delete==0]\n",
    "\treturn cleaned_data\n",
    "\n",
    "merged_data = pd.concat((male_data,female_data),axis=0)\n",
    "\n",
    "merged_data['name'] = merged_data['name'].apply(clean_data)\n",
    "merged_data['count_words'] = merged_data['name'].str.split().apply(len)\n",
    "\n",
    "cleaned_data = remove_records(merged_data)\n",
    "\n",
    "indian_cleaned_data = cleaned_data[['name','count_words']].drop_duplicates(subset='name',keep='first')\n",
    "indian_cleaned_data['label'] = 'indian'\n",
    "\n",
    "len(indian_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker(‘en_US’)\n",
    "fake.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_indian_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import Callback\n",
    "np.random.seed(42)\n",
    "def char_encoded_representation(data,tokenizer,vocab_size,max_len):\n",
    "  char_index_sentences = tokenizer.texts_to_sequences(data)\n",
    "  sequences = [to_categorical(x, num_classes=vocab_size) for x in   char_index_sentences]\n",
    "  X = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "  return X\n",
    "\n",
    "def build_model(hidden_units,max_len,vocab_size):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(hidden_units,input_shape=(max_len,vocab_size)))\n",
    "  model.add(Dense(1, activation=’sigmoid’))\n",
    "  model.compile(loss=’binary_crossentropy’, optimizer=’adam’,   metrics=[‘accuracy’])\n",
    "  print(model.summary())\n",
    "  return model\n",
    "model = build_model(100,max_len,vocab_size)\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64,callbacks=myCallback(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
